{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a958cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import utils\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "from transformers.trainer_utils import set_seed\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import AutoConfig, AutoModelWithHeads\n",
    "from transformers.adapters.composition import Stack\n",
    "from transformers import TrainingArguments, EvalPrediction, AdapterTrainer\n",
    "from utils.evaluation import compute_pearsonr\n",
    "\n",
    "\n",
    "dateTimeObj = datetime.now()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca93bf",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba95ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load data \"\"\"\n",
    "\n",
    "train_data, val_data, test_data = utils.load_wassa_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c213ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" get task labels \"\"\"\n",
    "\n",
    "prediction_task = 'empathy'\n",
    "\n",
    "train_labels = list(train_data[prediction_task].values)\n",
    "val_labels = list(val_data[prediction_task].values)\n",
    "test_labels = list(test_data[prediction_task].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d6a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare dataset for training: feature encodings \"\"\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "train_encodings = tokenizer(list(train_data['essay'].values), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_data['essay'].values), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data['essay'].values), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfe14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" setup torch dataset \"\"\"\n",
    "\n",
    "class WassaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = WassaDataset(train_encodings, train_labels)\n",
    "val_dataset = WassaDataset(val_encodings, val_labels)\n",
    "test_dataset = WassaDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e85f0",
   "metadata": {},
   "source": [
    "# Stacking Emotion Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263aafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbalguri/.local/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:274: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/home/nbalguri/.local/lib/python3.9/site-packages/transformers/adapters/models/roberta/adapter_model.py:252: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" init model \"\"\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=1,\n",
    "    hidden_dropout_prob=.01,\n",
    ")\n",
    "model = AutoModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9aa98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d12f26f18d043f3af2ded9cf14898f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" load emotion adapter, using: https://huggingface.co/AdapterHub/roberta-base-pf-emotion \"\"\"\n",
    "\n",
    "emotion_adapter = model.load_adapter('AdapterHub/roberta-base-pf-emotion', source=\"hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966bb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" add adapter for emotion prediction task \"\"\"\n",
    "\n",
    "adapter_name = \"EMP_emotion_stack\" if prediction_task == 'empathy' else 'DIS_emotion_stack'\n",
    "\n",
    "model.add_adapter(adapter_name)\n",
    "model.add_classification_head(adapter_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c663e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" activate adapter stack \"\"\"\n",
    "\n",
    "model.active_adapters = Stack(emotion_adapter, adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" activate adapter for training \"\"\"\n",
    "\n",
    "model.train_adapter([adapter_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4a4f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" training arguments \"\"\"\n",
    "\n",
    "approach = 'emotion-stack'\n",
    "training_output_dir = f\"./training_output/{approach}/{prediction_task}_{dateTimeObj.hour}{dateTimeObj.minute}-{dateTimeObj.day}-{dateTimeObj.month}\"\n",
    "num_train_epochs=20\n",
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=8\n",
    "metric_for_best_model='eval_pearsonr'\n",
    "warmup_steps=1000\n",
    "weight_decay=0.1\n",
    "learning_rate=1e-04\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    seed=RANDOM_SEED,\n",
    "    output_dir=training_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "     warmup_steps=warmup_steps,\n",
    "     weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    disable_tqdm=False,\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f3a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" setup trainer \"\"\"\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_pearsonr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e212f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbalguri/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1860\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4660\n",
      "  Number of trainable parameters = 2081095\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2161' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2161/4660 2:59:45 < 3:28:03, 0.20 it/s, Epoch 9.27/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearsonr</th>\n",
       "      <th>Pearsonr Scipy</th>\n",
       "      <th>Pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>19.310400</td>\n",
       "      <td>16.894705</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.034259</td>\n",
       "      <td>0.575144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>16.912900</td>\n",
       "      <td>14.828650</td>\n",
       "      <td>-0.023100</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>0.705845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>12.510200</td>\n",
       "      <td>9.373271</td>\n",
       "      <td>-0.039700</td>\n",
       "      <td>-0.039704</td>\n",
       "      <td>0.515929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.501000</td>\n",
       "      <td>3.529942</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.506325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.256400</td>\n",
       "      <td>3.434055</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.116411</td>\n",
       "      <td>0.056074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.684500</td>\n",
       "      <td>3.318197</td>\n",
       "      <td>0.223800</td>\n",
       "      <td>0.223845</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.132900</td>\n",
       "      <td>3.241638</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.300106</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.234400</td>\n",
       "      <td>3.046634</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.356980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.968300</td>\n",
       "      <td>2.955862</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.380938</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.655200</td>\n",
       "      <td>3.408524</td>\n",
       "      <td>0.403600</td>\n",
       "      <td>0.403595</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.958500</td>\n",
       "      <td>2.870115</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>0.410414</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.008500</td>\n",
       "      <td>2.849987</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.435029</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.815000</td>\n",
       "      <td>3.047717</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.436917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.698900</td>\n",
       "      <td>2.900609</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.445526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.887500</td>\n",
       "      <td>3.048663</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.442782</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.712300</td>\n",
       "      <td>2.804452</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>0.455392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.601100</td>\n",
       "      <td>2.878519</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.428387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.467600</td>\n",
       "      <td>2.777989</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.445500</td>\n",
       "      <td>3.277865</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.461379</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.740500</td>\n",
       "      <td>3.123713</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.449369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.526500</td>\n",
       "      <td>2.877125</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.459128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.504000</td>\n",
       "      <td>2.811765</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.443470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.307900</td>\n",
       "      <td>2.824198</td>\n",
       "      <td>0.435200</td>\n",
       "      <td>0.435216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.222200</td>\n",
       "      <td>2.952366</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.413698</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.427800</td>\n",
       "      <td>2.789178</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.451763</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.147600</td>\n",
       "      <td>2.927683</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.280600</td>\n",
       "      <td>2.908759</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.436617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.469000</td>\n",
       "      <td>2.959205</td>\n",
       "      <td>0.434600</td>\n",
       "      <td>0.434579</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.132500</td>\n",
       "      <td>3.223140</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.424484</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.097500</td>\n",
       "      <td>2.962324</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.432541</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.021600</td>\n",
       "      <td>2.926787</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.443679</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.101200</td>\n",
       "      <td>3.047616</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.421786</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.124300</td>\n",
       "      <td>2.871047</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.433252</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.045100</td>\n",
       "      <td>3.044910</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.446156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>2.890940</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.464934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.792700</td>\n",
       "      <td>3.121008</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.454191</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.926200</td>\n",
       "      <td>3.071691</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.768900</td>\n",
       "      <td>2.953402</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.446073</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.620400</td>\n",
       "      <td>3.072580</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.431848</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.686100</td>\n",
       "      <td>3.102756</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.383810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.600400</td>\n",
       "      <td>3.308728</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.408831</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>3.127067</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.431438</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.247700</td>\n",
       "      <td>3.163427</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.421655</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-50/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-150/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-200/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-250/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-300/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-350/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-400/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-450/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-500/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-550/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-600/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-650/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-700/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-750/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-800/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-850/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-900/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-950/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1000/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1050/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1150/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1200/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1250/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1300/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1350/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1400/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1450/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1500/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1550/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1600/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1650/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1700/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1750/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1800/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1850/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1900/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-1950/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2000/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2050/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2100/EMP_emotion_stack/pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 270\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/adapter_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/emotion/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/head_config.json\n",
      "Module weights saved in ./training_output/emotion-stack/empathy_154-7-4/checkpoint-2150/EMP_emotion_stack/pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "\"\"\" train \"\"\"\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960fc71",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" output eval metrics from best model \"\"\"\n",
    "\n",
    "#trainer.model.cuda()\n",
    "eval_output = trainer.evaluate()\n",
    "eval_result = eval_output[metric_for_best_model]\n",
    "\n",
    "pd.DataFrame({'metric':list(eval_output.keys()), 'value': list(eval_output.values())}, columns=['metric', 'value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cccea",
   "metadata": {},
   "source": [
    "# Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a108eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" make predictions on test dataset \"\"\"\n",
    "\n",
    "p = trainer.predict(test_dataset)\n",
    "preds = p.predictions[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c108d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" save predictions \"\"\"\n",
    "\n",
    "pred_file = f\"{approach}-{metric_for_best_model}-{round(eval_result * 100, 4)}_{dateTimeObj.hour}{dateTimeObj.minute}-{dateTimeObj.day}-{dateTimeObj.month}.tsv\"\n",
    "pred_path = f'./predictions/{prediction_task}/{pred_file}'\n",
    "os.makedirs(f'./predictions/{prediction_task}/', exist_ok=True)\n",
    "pd.Series(preds).to_csv(pred_path, sep='\\t', header=False, index=False)\n",
    "print(\"saved predictions to\",pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa8f64",
   "metadata": {},
   "source": [
    "# Save the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f24ff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_adapter(f\"./trained_adapters/{adapter_name}\", adapter_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
